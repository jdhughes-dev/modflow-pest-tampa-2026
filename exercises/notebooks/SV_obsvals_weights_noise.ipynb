{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Set observation values, weights and noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pyemu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "Load the \"observation data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_csv_fname = os.path.join(\n",
    "    \"..\", \"models\", \"synthetic-valley-truth-advanced-monthly\", \"raw_obs.csv\"\n",
    ")\n",
    "assert os.path.exists(obs_csv_fname)\n",
    "obsdf = pd.read_csv(obs_csv_fname, index_col=0, parse_dates=True)\n",
    "obsdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "Since we know the \"true\" values for our forecast, we will load them up and put the truth values in the control file to make plotting easier (and more interesting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fore_csv_fname = os.path.join(\n",
    "    \"..\", \"models\", \"synthetic-valley-truth-advanced-monthly\", \"swgw-longterm-means.csv\"\n",
    ")\n",
    "assert os.path.exists(fore_csv_fname)\n",
    "foredf = pd.read_csv(fore_csv_fname, index_col=0)\n",
    "foredf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "working_d = \"model_and_pest_files\"\n",
    "assert os.path.exists(working_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "Models are always going to be low-pass filters compared to the complex natural systems that generated the observations.  So its usually a good idea to filter out high freq signal components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "smoothed = obsdf.rolling(window=18, center=True, min_periods=1).mean()\n",
    "for col in smoothed.columns:\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(6, 3))\n",
    "    smoothed.loc[:, col].plot(ax=ax, c=\"g\")\n",
    "    obsdf.loc[:, col].plot(ax=ax, c=\"m\")\n",
    "    ax.set_title(col, loc=\"left\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "Now load the control file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "pst = pyemu.Pst(os.path.join(working_d, \"pest.pst\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = pst.observation_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "First set the truth values for the forecasts (just for plotting later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in foredf.columns:\n",
    "    fobs = obs.loc[obs.obsnme.str.contains(col), :]\n",
    "    for name, q in zip(fobs.obsnme, fobs.quantity):\n",
    "        obs.loc[name, \"obsval\"] = foredf.loc[q, col]\n",
    "        print(col, q)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "Now for the tricky part: we need to find each simulated output that we have an observed counterpart for.  In practice, this usually requires some bespoke code/hackery (we are also going to set \"observed\" values from the forecast period, just so we can plot it later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "nnobs = obs.loc[pd.notna(obs.usecol), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefixes = [\n",
    "    \"wt\",\n",
    "    \"aq\",\n",
    "    \"lake-stage\",\n",
    "    \"lake-swgw\",\n",
    "    \"riv-flow\",\n",
    "    \"riv-swgw\",\n",
    "    \"diff1\",\n",
    "    \"diff0\",\n",
    "]\n",
    "for prefix in prefixes:\n",
    "    uobs = nnobs.loc[nnobs.usecol.str.contains(prefix), :].copy()\n",
    "    print(prefix, uobs.shape)\n",
    "    uobs[\"datetime\"] = pd.to_datetime(uobs.datetime)\n",
    "    for usecol in uobs.usecol.unique():\n",
    "        uuobs = uobs.loc[uobs.usecol == usecol, :].copy()\n",
    "        for dt, name in zip(uuobs.datetime, uuobs.obsnme):\n",
    "            oval = smoothed.loc[dt, usecol]\n",
    "            obs.loc[name, \"obsval\"] = oval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "now we need to set the weights and expected noise for each observation datum:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs[\"weight\"] = 0.0\n",
    "obs[\"standard_deviation\"] = np.nan\n",
    "obs[\"lower_bound\"] = np.nan\n",
    "obs[\"upper_bound\"] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_dict = {}\n",
    "hist_prefixes = [\"wt\", \"aq\", \"lake-stage\", \"diff1\", \"diff0\"]\n",
    "for prefix in hist_prefixes:\n",
    "    uobs = nnobs.loc[nnobs.usecol.str.startswith(prefix), :].copy()\n",
    "    print(uobs.usecol.unique())\n",
    "    uobs[\"datetime\"] = pd.to_datetime(uobs.datetime)\n",
    "    hist_uobs = uobs.loc[uobs.datetime.dt.year < 2015, :]\n",
    "    obs.loc[hist_uobs.obsnme, \"datetime\"] = hist_uobs.datetime\n",
    "    if \"lake\" in prefix:\n",
    "        obs.loc[hist_uobs.obsnme, \"weight\"] = 3.0\n",
    "        obs.loc[hist_uobs.obsnme, \"standard_deviation\"] = 0.3\n",
    "    elif \"diff\" in prefix:\n",
    "        print(prefix)\n",
    "        obs.loc[hist_uobs.obsnme, \"weight\"] = [\n",
    "            5.0 if oval > 0.1 else 1.0 for oval in hist_uobs.obsval\n",
    "        ]\n",
    "        obs.loc[hist_uobs.obsnme, \"standard_deviation\"] = [\n",
    "            max(0.01, oval * 0.1) for oval in hist_uobs.obsval\n",
    "        ]\n",
    "        obs.loc[hist_uobs.obsnme, \"lower_bound\"] = 0.0\n",
    "    else:\n",
    "        obs.loc[hist_uobs.obsnme, \"weight\"] = 2.0\n",
    "        obs.loc[hist_uobs.obsnme, \"standard_deviation\"] = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "if this is an advanced model, we can also use riv-flow information for history matching (one benefit of a more complex model) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"riv-flow\" in obs.usecol.unique():\n",
    "    uobs = nnobs.loc[nnobs.usecol == \"riv-flow\", :].copy()\n",
    "    uobs[\"datetime\"] = pd.to_datetime(uobs.datetime)\n",
    "    hist_uobs = uobs.loc[uobs.datetime.dt.year < 2015, :].copy()\n",
    "    hist_uobs[\"standard_deviation\"] = [\n",
    "        max(0.2, oval * 0.1) for oval in np.abs(hist_uobs.obsval.values)\n",
    "    ]\n",
    "    hist_uobs.loc[hist_uobs.obsnme, \"weight\"] = 1 / hist_uobs.standard_deviation.values\n",
    "    obs.loc[hist_uobs.obsnme, \"standard_deviation\"] = (\n",
    "        hist_uobs.standard_deviation.values\n",
    "    )\n",
    "    obs.loc[hist_uobs.obsnme, \"weight\"] = hist_uobs.weight.values\n",
    "    obs.loc[hist_uobs.obsnme, \"datetime\"] = hist_uobs.datetime\n",
    "\n",
    "    print(hist_uobs.loc[:, [\"obsval\", \"standard_deviation\", \"weight\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "pst.nnz_obs_groups"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "Set noptmax to 0, save the control file and do a test run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "pst.control_data.noptmax = 0\n",
    "pst.write(os.path.join(working_d, \"pest.pst\"), version=2)\n",
    "pyemu.os_utils.run(\"pestpp-ies pest.pst\", cwd=working_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "Now we are going to generate some autocorrelated timeseries noise to use in the history matching:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "nzobs = obs.loc[obs.weight > 0, :].copy()\n",
    "obs[\"distance\"] = np.nan\n",
    "grps = nzobs.obgnme.unique()\n",
    "grps.sort()\n",
    "struct_dict = {}\n",
    "for grp in grps:\n",
    "    gobs = nzobs.loc[nzobs.obgnme == grp, :].copy()\n",
    "    gobs[\"datetime\"] = pd.to_datetime(gobs.datetime)\n",
    "    gobs[\"distance\"] = (gobs.datetime - gobs.datetime.min()).dt.days\n",
    "    obs.loc[gobs.obsnme, \"distance\"] = gobs.distance\n",
    "    v = pyemu.geostats.ExpVario(contribution=1.0, a=365 * 20)\n",
    "    gs = pyemu.geostats.GeoStruct(variograms=v, name=grp)\n",
    "    struct_dict[gs] = gobs.obsnme.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = pyemu.helpers.autocorrelated_draw(pst, struct_dict, num_reals=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "We know from expert knowledge that the vertical head differences are never negative between the water table and aquifer, so let's repair any noise realizations that have that condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "lbnd = obs.loc[pd.notna(obs.lower_bound), \"lower_bound\"]\n",
    "for name, bnd in zip(lbnd.index, lbnd.values):\n",
    "    vals = noise.loc[:, name].values\n",
    "    vals[vals < bnd] = bnd\n",
    "    noise.loc[:, name] = vals\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "Save the noise ensemble, tell ies about it, and an noptmax=-2 test run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise.to_csv(os.path.join(working_d, \"noise.csv\"))\n",
    "pst.pestpp_options = {\"ies_par_en\": pst.pestpp_options[\"ies_par_en\"]}\n",
    "pst.pestpp_options[\"ies_obs_en\"] = \"noise.csv\"\n",
    "pst.control_data.noptmax = -2\n",
    "pst.write(os.path.join(working_d, \"pest.pst\"), version=2)\n",
    "pyemu.os_utils.run(\"pestpp-ies pest.pst\", cwd=working_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "As you can see from the phi group summary, we need some rebalanced weights.  One way to do this is the the ies_phi_factor_file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "phi_factors = {\"lake\": 0.2, \"aq\": 0.3, \"wt\": 0.3, \"diff\": 0.2}\n",
    "if \"riv-flow\" in obs.usecol.unique():\n",
    "    phi_factors = {\"lake\": 0.15, \"aq\": 0.25, \"wt\": 0.25, \"diff\": 0.15, \"riv-flow\": 0.2}\n",
    "# phi_factors[\"diff\"] = 1e-20\n",
    "ser = pd.Series(phi_factors)\n",
    "ser.to_csv(os.path.join(working_d, \"phi_facs.csv\"), index=True, header=False)\n",
    "pst.pestpp_options[\"ies_phi_factor_file\"] = \"phi_facs.csv\"\n",
    "pst.write(os.path.join(working_d, \"pest.pst\"), version=2)\n",
    "pyemu.os_utils.run(\"pestpp-ies pest.pst\", cwd=working_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
